"""
Local LLM Model Handler
Handles interactions with local LLM models (using Ollama)
"""

from typing import List, Dict, Optional
from pathlib import Path
from sentence_transformers import SentenceTransformer
from loguru import logger
import requests
import json


class LocalModel:
    """Local LLM Handler using Ollama"""
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        ollama_url: str = "http://localhost:11434"
    ):
        """
        Initialize Local LLM with Ollama
        
        Args:
            model_name: Ollama model name (e.g., 'qwen2.5:3b')
            embedding_model: HuggingFace model for embeddings
            ollama_url: Ollama API URL
        """
        from app.config import get_settings
        settings = get_settings()
        
        self.model_name = model_name or settings.local_model_type or "qwen2.5:3b"
        self.ollama_url = ollama_url
        self.api_endpoint = f"{ollama_url}/api/generate"
        self.ollama_available = False  # Initialize as False
        
        # Test Ollama connection
        try:
            response = requests.get(f"{ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                self.ollama_available = True  # Set to True on success
                logger.info(f"Connected to Ollama at {ollama_url}")
                models = response.json().get("models", [])
                model_names = [m.get("name") for m in models]
                logger.info(f"Available models: {model_names}")
                
                if not any(self.model_name in name for name in model_names):
                    logger.warning(f"Model {self.model_name} not found. Please run: ollama pull {self.model_name}")
                    self.ollama_available = False  # Model not found
            else:
                logger.warning(f"Failed to connect to Ollama: {response.status_code}")
        except Exception as e:
            logger.warning(f"Could not connect to Ollama: {e}. Make sure Ollama is running.")
        
        # Initialize embedding model
        logger.info(f"Loading embedding model: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        
        logger.info(f"Local Model initialized with Ollama model: {self.model_name}")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """
        Generate response using Ollama
        
        Args:
            prompt: User prompt
            system_prompt: System instruction
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            **kwargs: Additional parameters
        
        Returns:
            Generated text response
        """
        try:
            # Check if Ollama is available
            if not self.ollama_available:
                logger.warning("Ollama not available, returning context-based summary")
                # Return a simple summary from the prompt
                return self._generate_simple_summary(prompt)
            
            # Prepare request payload
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                }
            }
            
            if system_prompt:
                payload["system"] = system_prompt
            
            # Make API request
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=120
            )
            response.raise_for_status()
            
            result = response.json().get("response", "").strip()
            logger.debug(f"Generated response: {result[:100]}...")
            
            return result
        
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            # Fallback to simple summary
            return self._generate_simple_summary(prompt)
    
    def _generate_simple_summary(self, prompt: str) -> str:
        """
        Generate a simple summary when Ollama is not available
        
        Args:
            prompt: The prompt containing context and question
        
        Returns:
            Simple formatted summary of the context
        """
        # Extract context documents from prompt
        lines = prompt.split('\n')
        
        summary_parts = []
        summary_parts.append("**ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìš”ì•½:**\n")
        
        # Find document sections
        doc_sections = []
        current_doc = []
        for line in lines:
            if line.startswith('[ë¬¸ì„œ'):
                if current_doc:
                    doc_sections.append('\n'.join(current_doc))
                current_doc = [line]
            elif current_doc:
                current_doc.append(line)
        
        if current_doc:
            doc_sections.append('\n'.join(current_doc))
        
        # Add document summaries
        for i, doc_section in enumerate(doc_sections[:3], 1):  # Max 3 documents
            # Extract first 200 characters of each document
            doc_lines = doc_section.split('\n')
            if doc_lines:
                header = doc_lines[0]
                content = ' '.join(doc_lines[1:])[:300]
                summary_parts.append(f"\n{header}")
                summary_parts.append(f"{content}...\n")
        
        summary_parts.append("\nğŸ’¡ **ì°¸ê³ **: ë” ìƒì„¸í•œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” Ollama ì„œë²„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        summary_parts.append("ìì„¸í•œ ì„¤ì¹˜ ë°©ë²•: https://ollama.ai/")
        
        return '\n'.join(summary_parts)
    
    def generate_with_context(
        self,
        question: str,
        context_docs: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 1500
    ) -> str:
        """
        Generate response with RAG context
        
        Args:
            question: User question
            context_docs: List of retrieved documents
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
        
        Returns:
            Generated answer
        """
        # Build context
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            etf_name = metadata.get("etf_name", "Unknown")
            date = metadata.get("date", "Unknown")
            source = metadata.get("source", "Unknown")
            
            context_parts.append(
                f"[ë¬¸ì„œ {i}] {etf_name} (ë‚ ì§œ: {date}, ì¶œì²˜: {source})\n{content}\n"
            )
        
        context_text = "\n".join(context_parts)
        
        # System prompt
        system_prompt = """ë‹¹ì‹ ì€ ETF(ìƒì¥ì§€ìˆ˜í€ë“œ) íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
1. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œì— ì—†ë‹¤ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ë¥¼ [ë¬¸ì„œ N] í˜•íƒœë¡œ ì¸ìš©í•˜ì„¸ìš”
4. íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹Œ ì •ë³´ ì œê³µì— ì´ˆì ì„ ë§ì¶”ì„¸ìš”
5. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”"""
        
        prompt = f"""ë‹¤ìŒì€ ê´€ë ¨ ETF ì •ë³´ì…ë‹ˆë‹¤:

{context_text}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        return self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
    
    def get_embedding(self, text: str) -> List[float]:
        """
        Get text embedding using SentenceTransformer
        
        Args:
            text: Text to embed
        
        Returns:
            Embedding vector
        """
        try:
            embedding = self.embedding_model.encode(
                text,
                convert_to_numpy=True
            )
            
            embedding_list = embedding.tolist()
            logger.debug(f"Generated embedding with dimension: {len(embedding_list)}")
            
            return embedding_list
        
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            raise
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Get embeddings for multiple texts
        
        Args:
            texts: List of texts to embed
        
        Returns:
            List of embedding vectors
        """
        try:
            embeddings = self.embedding_model.encode(
                texts,
                convert_to_numpy=True,
                show_progress_bar=True
            )
            
            embeddings_list = [emb.tolist() for emb in embeddings]
            logger.debug(f"Generated {len(embeddings_list)} embeddings")
            
            return embeddings_list
        
        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            raise


# Example usage
if __name__ == "__main__":
    logger.info("Testing Local Model...")
    
    # Note: This requires a valid GGUF model file
    # Example: download a model from HuggingFace
    # e.g., TheBloke/Mistral-7B-Instruct-v0.2-GGUF
    
    try:
        model = LocalModel(
            model_path="./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            n_gpu_layers=0  # Set > 0 if you have GPU
        )
        
        # Test generation
        response = model.generate(
            prompt="ETFë€ ë¬´ì—‡ì¸ê°€ìš”?",
            temperature=0.7
        )
        print(f"Response: {response}")
        
        # Test embedding
        embedding = model.get_embedding("KODEX 200 ETF")
        print(f"Embedding dimension: {len(embedding)}")
        
    except Exception as e:
        logger.error(f"Error: {e}")
        print("Note: You need to download a GGUF model file first")
